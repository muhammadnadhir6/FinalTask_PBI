# -*- coding: utf-8 -*-
"""Muhammad Nadhir_Credit Risk Assesment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YOjDlRpiv5Us_pmMTYAjSfj4vkuwHe9g

## Credit Risk Assessment

### Objective

### 1. Create a credit score based on the weighting of the logistic regression
### 2. Interpretation of important features

### Import Library and Data
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 200)

df = pd.read_csv('loan_data_2007_2014.csv')

"""### Data Understanding"""

df.info()

"""* There are some columns that have null values so it needs to  **Cleansing**"""

df.sample(5)

"""* Drop column`Unnamed: 0` because it's just a sequence of indexes in the dataset

## Data Cleansing
"""

df_clean = df.copy()

df_clean.drop(columns=['Unnamed: 0'], inplace = True)

cat = df_clean.select_dtypes(include = 'object')
numerical = df_clean.select_dtypes(exclude = 'object')
cat_cols = cat.columns.to_list()
numerical_cols = numerical.columns.to_list()

df_clean[numerical_cols].describe()

"""* Seen some features have more null values so they need to be drop"""

df_clean[cat_cols].describe().T

"""* Some categorical features have data form that should be of type datetime but are still of type object in this dataset<br>
* Some categorical features also have too many unique values and only have one unique value so it will be drop

### Check Duplicated Values
"""

df_clean.duplicated().sum()

"""* Seen there is no duplicate data so that one row already represents one individual

### Check Missing Value
"""

# Total null values
total_null = df_clean.isnull().sum()
percent_missing = df_clean.isnull().sum() * 100/ len(df)
dtypes = [df_clean[col].dtype for col in df_clean.columns]
df_missing_value = pd.DataFrame({'total_null': total_null,
                                'data_type': dtypes,
                                'percent_missing': percent_missing})
df_missing_value.sort_values('percent_missing', ascending = False,inplace = True)
missing_value = df_missing_value[df_missing_value['percent_missing']>0].reset_index()

plt.style.use('default')
fig, ax = plt.subplots(figsize=(20, 20))
plt.title("Missing Value Ratio", fontsize=20, color='black', weight='bold',pad = 50)
#plt.text(x=0, y=-2, s="Persentase tertinggi karyawan resign Data Analyst 50%  ,disusul Sofware Engineer (Front End) 61% dan Product Design (UI & UX) 62%\nSoftware Engineer Front End maupun Back End memiliki jumlah karywan resign tertinggi ",
         #fontsize=15, fontstyle='italic')
sns.barplot(y='index', x='percent_missing', data=missing_value, edgecolor='black',color = '#6BBD9B')
plt.bar_label(ax.containers[0], padding=2,fmt='%.2f%%')
plt.xlabel('Percentage (%)', fontsize=14)
plt.ylabel('Feature Name', fontsize=14)
sns.despine()

"""## Handling Missing Value"""

# Drop feature that have more than 50% missing value
col_full_null = df_missing_value.loc[df_missing_value['percent_missing']> 50].index.tolist()
df_clean.drop(columns=col_full_null, inplace = True)

# Feature `tot_coll_amt`,`tot_cur_bal`,`total_rev_hi_lim` replace missing value with "0" because asumption that customer didn't borrowed again
for col in ['tot_coll_amt','tot_cur_bal','total_rev_hi_lim']:
    df_clean[col] = df_clean[col].fillna(0)

# Numerical columns replace missing value with "Median"
for col in df_clean.select_dtypes(exclude = 'object'):
    df_clean[col] = df_clean[col].fillna(df_clean[col].median())
df_clean.isnull().sum()

# Categorical columns replace missing value with "Mode"
for col in df_clean.select_dtypes(include = 'object'):
    df_clean[col] = df_clean[col].fillna(df_clean[col].mode().iloc[0])
df_clean.isnull().sum()

"""## Check Target Value

### Target Feature : `loan_status`
"""

df_clean['loan_status'].unique()

df_clean['loan_status'].value_counts()

"""* There are 9 unique values in the loan_status column that will be the target model.<br>
* Divided into two groups such as binary classification, namely "good loans" with the number 1 and "bad_loan" with the number 0<br>
* good_loan is defined as having a loan status of `Current`, `Fully Paid` , and `In Grace Period`<br>
* bad loan is defined as having a loan status other than good loan
"""

good_loan = ['Current','Fully Paid','In Grace Period']
df_clean['loan_status'] = np.where(df_clean['loan_status'].isin(good_loan),1,0)
df_clean['loan_status'].value_counts()

"""### Drop Unnecesary Column

Removal of features such as features that are unique id, in the form of free text
"""

df_clean.drop(columns=['member_id','url','title','addr_state','zip_code','policy_code','application_type','emp_title'], inplace = True)

df_cleaned = df_clean.copy()

"""## Feature Engineering

### Date Time Feature : 'earliest_cr_line','last_credit_pull_d','last_pymnt_d','issue_d','next_pymnt_d'
"""

df_cleaned['earliest_cr_line'] = pd.to_datetime(df_cleaned['earliest_cr_line'], format = '%b-%y')
df_cleaned['last_credit_pull_d'] = pd.to_datetime(df_cleaned['last_credit_pull_d'], format = '%b-%y')
df_cleaned['last_pymnt_d'] = pd.to_datetime(df_cleaned['last_pymnt_d'], format = '%b-%y')
df_cleaned['issue_d'] = pd.to_datetime(df_cleaned['issue_d'], format = '%b-%y')
df_cleaned['next_pymnt_d'] = pd.to_datetime(df_cleaned['next_pymnt_d'], format = '%b-%y')

df_cleaned[['earliest_cr_line','last_credit_pull_d','last_pymnt_d','issue_d','next_pymnt_d']].head()

"""Adding New Feature :<br>
* `pymnt_time` = the number of month between `next_pymnt_d` and `last_pymnt_d`
* `credit_pull_year` = the number of year between `earliest_cr_line` and `last_credit_pull_d`
"""

def diff_month(d1, d2):
    return (d1.year - d2.year) * 12 + d1.month - d2.month
def diff_year(d1, d2):
    return (d1.year - d2.year)

df_cleaned['pymnt_time'] = df_cleaned.apply(lambda x: diff_month(x.next_pymnt_d, x.last_pymnt_d), axis=1)
df_cleaned['credit_pull_year'] = df_cleaned.apply(lambda x: diff_year(x.last_credit_pull_d, x.earliest_cr_line), axis=1)

df_cleaned[df_cleaned['pymnt_time']<0][['next_pymnt_d','last_pymnt_d','pymnt_time']]

df_cleaned[df_cleaned['credit_pull_year']<0][['earliest_cr_line','last_credit_pull_d','credit_pull_year']]

"""* There is a negative value in the `pymnt_time` feature, the value will be replaced with 0 because it assumes that the customer does not have a bill to make a payment

* There is an false input in the `earliest_cr_line` feature, resulting in a negative `credit_pull_year` value so that the line with that value will be replaced with the maximum value of the `credit_pull_year `feature
"""

df_cleaned.loc[df_cleaned['pymnt_time'] < 0,'pymnt_time'] = 0
df_cleaned.loc[df_cleaned['credit_pull_year'] < 0,'credit_pull_year'] = df_cleaned['credit_pull_year'].max()

"""### Drop column that already extract"""

df_cleaned.drop(columns=['issue_d','earliest_cr_line','next_pymnt_d','last_pymnt_d','last_credit_pull_d'], inplace = True)

"""### Date Time Feature : `term` to integer"""

df_cleaned['term'].value_counts()

df_cleaned['term'] = df_cleaned['term'].apply(lambda term: int(term[:3]))

df_cleaned['term'].value_counts()

df_cleaned.shape

df_cleaned.head()

"""### EDA"""

df_eda = df_cleaned.copy()

df_eda.drop(columns=['id'], inplace = True)

num = df_eda.select_dtypes(include='number').columns
cat = df_eda.select_dtypes(include='object').columns

plt.figure(figsize=(24,24))
sns.heatmap(df_eda.corr(),annot=True,fmt='.3f')

"""* It appears that there are several independent features that are highly correlated with each other so that it can lead to biased results if left unchecked.
* Decided to remove features that have a correlation of more than 0.8
"""

corr_matrix = df_eda.corr().abs()
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))
high_multicol = [column for column in upper.columns if any(upper[column] > 0.8)]

high_multicol

df_eda.drop(high_multicol, axis=1, inplace=True)

df_eda[cat].nunique()

"""* `grade` and `sub_grade` have similar interpretations, so decided to remove the `sub_grade` feature because it is already represented by `grade`"""

df_eda.drop(['sub_grade'], axis=1, inplace=True)

"""## Univariate Analysis

### Numeric Feature
"""

len(df_eda.select_dtypes(include='number').columns)

num = df_eda.select_dtypes(include='number').columns
cat = df_eda.select_dtypes(include='object').columns

plt.figure(figsize=(24,28))
for i in range(0,len(num)):
    plt.subplot(10,4,i+1)
    sns.kdeplot(x=df_eda[num[i]], palette='viridis', shade=True)
    plt.title(num[i], fontsize=20)
    plt.xlabel(' ')
    plt.tight_layout()

"""* The majority of numerical features are not normally distributed"""

plt.figure(figsize=(20,20))
for i in range(0,len(cat)):
    plt.subplot(5,2,i+1)
    sns.countplot(y=df_eda[cat[i]], orient = 'h',palette='viridis')
    plt.title(cat[i])
    plt.xlabel(' ')
    plt.tight_layout()

"""Some Insights obtained in the graph above:<br>
1. Feature `grade` is dominated by grade B
2. The `emp_length` feature is dominated by emp_length >10 years
3. The `home_ownership` feature is dominated by mortgage
4. The `verification` status feature is dominated by verified
5. The `purpose` feature is dominated by debt_consolidation
6. `initial_list_status` feature is dominated by f
<br>
* The `pymnt_plan` feature has a problem because the whole data only contains one unique value so decided to remove this feature

### Bivariate Analysis
"""

plt.figure(figsize=(20,20))
for i in range(0,len(cat)):
    plt.subplot(5,2,i+1)
    sns.countplot(y=df_eda[cat[i]], palette='viridis', hue=df_eda['loan_status'])
    plt.title(cat[i])
    plt.xlabel(' ')
    plt.tight_layout()

df_eda.drop(['pymnt_plan'], axis=1, inplace=True)

"""### Feature Engineering with Weight of Evidence and Information Value (IV)"""

df_fe = df_eda.copy()

# Create Function for Weight of Evidence and Invormation Value

def woe(df, feature_name):
    feature_name = df.groupby(feature_name).agg(num_observation=('loan_status','count'),good_loan_prob=('loan_status','mean')).reset_index()
    feature_name['grade_proportion'] = feature_name['num_observation']/feature_name['num_observation'].sum()
    feature_name['num_good_loan'] = feature_name['grade_proportion'] * feature_name['num_observation']
    feature_name['num_bad_loan'] = (1-feature_name['grade_proportion']) * feature_name['num_observation']
    feature_name['good_loan_prop'] = feature_name['num_good_loan'] / feature_name['num_good_loan'].sum()
    feature_name['bad_loan_prop'] = feature_name['num_bad_loan'] / feature_name['num_bad_loan'].sum()
    feature_name['weight_of_evidence'] = np.log(feature_name['good_loan_prop'] / feature_name['bad_loan_prop'])
    feature_name = feature_name.sort_values('weight_of_evidence').reset_index(drop=True)
    feature_name['information_value'] = (feature_name['good_loan_prop']-feature_name['bad_loan_prop']) * feature_name['weight_of_evidence']
    feature_name['information_value'] = feature_name['information_value'].sum()

    #Show
    feature_name = feature_name.drop(['grade_proportion','num_good_loan','num_bad_loan','good_loan_prop','bad_loan_prop'],axis = 1)
    return feature_name

def dist(feature):
    plt.figure(figsize=(4,2))
    sns.violinplot(df_fe[feature],color='blue')
    print('number of unique values :',df_fe[feature].nunique())
    print('Distribution :')
    print(df_fe[feature].describe().T)

"""![Invormation Value.png](attachment:4ccf1301-2a2d-4571-900e-8134a81f6463.png)

* We will decide whether the feature will be used or will be remove or adjusted based on the information value rules above

### WOE Categoric Feature

#### WOE: `Grade`
"""

woe(df_fe,'grade')

"""#### WOE: `emp_length`"""

woe(df_fe,'emp_length')

"""#### WOE: `home_ownership`"""

woe(df_fe,'home_ownership')

"""#### WOE: `verification_status`"""

woe(df_fe,'verification_status')

"""Low information value,`verification_status` will drop

#### WOE: `purpose`
"""

woe(df_fe,'purpose')

"""#### WOE: `initial_list_status`"""

woe(df_fe,'initial_list_status')

"""#### WOE: `term`"""

woe(df_fe,'term')

"""### WOE Numeric Feature

#### WOE: `loan_amnt`
"""

dist('loan_amnt')

df_fe['loan_amnt_woe'] = pd.cut(df_fe['loan_amnt'],10)
woe(df_fe,'loan_amnt_woe')

"""#### WOE: `int_rate`"""

dist('int_rate')

df_fe['int_rate_woe'] = pd.cut(df_fe['int_rate'],10)
woe(df_fe,'int_rate_woe')

"""#### WOE: `annual_inc`"""

dist('annual_inc')

df_fe['annual_inc_fc'] = np.where((df_fe['annual_inc']>=0)&(df_fe['annual_inc']<=200000),'low_income',
                               np.where((df_fe['annual_inc']>200000)&(df_fe['annual_inc']<=1500000),'med_income','high_income'))
woe(df_fe,'annual_inc_fc')

"""* Information value`annual_inc_woe` to high, We will drop

#### WOE: `dti`
"""

dist('dti')

df_fe['dti_woe'] = pd.cut(df_fe['dti'],10)
woe(df_fe,'dti_woe')

"""#### WOE: `delinq_2yrs`"""

dist('delinq_2yrs')

df_fe['delinq_2yrs_woe'] = pd.cut(df_fe['delinq_2yrs'],10)
woe(df_fe,'delinq_2yrs_woe')

"""* Information value `delinq_2yrs_woe` to high,we will drop

#### WOE: `inq_last_6mths`
"""

dist('inq_last_6mths')

df_fe['inq_last_6mths_woe'] = pd.cut(df_fe['inq_last_6mths'],10)
woe(df_fe,'inq_last_6mths_woe')

"""* Information value `inq_last_6mths_woe` to high,we will drop

#### WOE: `open_acc`
"""

dist('open_acc')

df_fe['open_acc_woe'] = pd.cut(df_fe['open_acc'],10)
woe(df_fe,'open_acc_woe')

"""#### WOE: `pub_rec`"""

dist('pub_rec')

df_fe['pub_rec_woe'] = pd.cut(df_fe['pub_rec'],10)
woe(df_fe,'pub_rec_woe')

"""* Information value `pub_rec_woe` to high,we will drop

#### WOE: `revol_bal`
"""

dist('revol_bal')

df_fe['revol_bal_fc'] = np.where((df_fe['revol_bal']>=0)&(df_fe['revol_bal']<=5000),0,
                               np.where((df_fe['revol_bal']>5000)&(df_fe['revol_bal']<=10000),1,
                               np.where((df_fe['revol_bal']>10000)&(df_fe['revol_bal']<=15000),2,3)))
woe(df_fe,'revol_bal_fc')

"""#### WOE: `revol_util`"""

dist('revol_util')

df_fe['revol_util_fc'] = np.where((df_fe['revol_util']>=0)&(df_fe['revol_util']<=20),0,
                                  np.where((df_fe['revol_util']>20)&(df_fe['revol_util']<=40),1,
                                           np.where((df_fe['revol_util']>40)&(df_fe['revol_util']<=60),2,
                                                    np.where((df_fe['revol_util']>60)&(df_fe['revol_util']<=80),3,4))))
woe(df_fe,'revol_util_fc')

"""#### WOE: `total_acc`"""

dist('total_acc')

df_fe['total_acc_woe'] = pd.cut(df_fe['total_acc'],7)
woe(df_fe,'total_acc_woe')

"""#### WOE: `out_prncp`"""

dist('out_prncp')

df_fe['out_prncp_woe'] = pd.cut(df_fe['out_prncp'],10)
woe(df_fe,'out_prncp_woe')

"""* Information value `out_prncp` to high,we will drop

#### WOE: `total_pymnt`
"""

dist('total_pymnt')

df_fe['total_pymnt_woe'] = pd.cut(df_fe['total_pymnt'],10)
woe(df_fe,'total_pymnt_woe')

"""#### WOE: `total_rec_int`"""

dist('total_rec_int')

df_fe['total_rec_int_woe'] = pd.cut(df_fe['total_rec_int'],10)
woe(df_fe,'total_rec_int_woe')

"""* Information value `total_rec_int` to high,we will drop

#### WOE: `total_rec_late_fee`
"""

dist('total_rec_late_fee')

df_fe['total_rec_late_fee_woe'] = pd.cut(df_fe['total_rec_late_fee'],10)
woe(df_fe,'total_rec_late_fee_woe')

"""* Information value `total_rec_late_fee` to high,we will drop

#### WOE: `recoveries`
"""

dist('recoveries')

df_fe['recoveries_woe'] = pd.cut(df_fe['recoveries'],10)
woe(df_fe,'recoveries_woe')

"""* Information value `recoveries` to high,we will drop

#### WOE: `last_pymnt_amnt`
"""

dist('last_pymnt_amnt')

df_fe['last_pymnt_amnt_fc'] = np.where((df_fe['last_pymnt_amnt']>=0)&(df_fe['last_pymnt_amnt']<=500),0,
                               np.where((df_fe['last_pymnt_amnt']>500)&(df_fe['last_pymnt_amnt']<=1000),1,
                               np.where((df_fe['last_pymnt_amnt']>1000)&(df_fe['last_pymnt_amnt']<=1500),2,
                                        np.where((df_fe['last_pymnt_amnt']>1500)&(df_fe['last_pymnt_amnt']<=3500),3,4))))
woe(df_fe,'last_pymnt_amnt_fc')

"""#### WOE: `collections_12_mths_ex_med`"""

dist('collections_12_mths_ex_med')

df_fe['collections_12_mths_ex_med_woe'] = pd.cut(df_fe['collections_12_mths_ex_med'],10)
woe(df_fe,'collections_12_mths_ex_med_woe')

"""* Information value `collections_12_mths_ex_med` to high,we will drop

#### WOE: `acc_now_delinq`
"""

dist('acc_now_delinq')

df_fe['acc_now_delinq_woe'] = pd.cut(df_fe['acc_now_delinq'],10)
woe(df_fe,'acc_now_delinq_woe')

"""* Information value `acc_now_delinq` to high,we will drop

#### WOE: `tot_coll_amt`
"""

dist('tot_coll_amt')

df_fe['tot_coll_amt_woe'] = pd.cut(df_fe['tot_coll_amt'],10)
woe(df_fe,'tot_coll_amt_woe')

"""* Information value `tot_coll_amt` to high,we will drop

#### WOE: `tot_cur_bal`
"""

dist('tot_cur_bal')

df_fe['tot_cur_bal_woe'] = pd.cut(df_fe['tot_cur_bal'],10)
woe(df_fe,'tot_cur_bal_woe')

"""* Information value `tot_cur_bal` to high,we will drop

#### WOE: `total_rev_hi_lim`
"""

dist('total_rev_hi_lim')

df_fe['total_rev_hi_lim_woe'] = pd.cut(df_fe['total_rev_hi_lim'],10)
woe(df_fe,'total_rev_hi_lim_woe')

"""* Information value `total_rev_hi_lim` to high,we will drop

#### WOE: `pymnt_time`
"""

dist('pymnt_time')

df_fe['pymnt_time_fc'] = np.where((df_fe['pymnt_time']>=0)&(df_fe['pymnt_time']<=1),0,
                               np.where((df_fe['pymnt_time']>1)&(df_fe['pymnt_time']<=6),1,
                               np.where((df_fe['pymnt_time']>6)&(df_fe['pymnt_time']<=12),2,3)))
woe(df_fe,'pymnt_time_fc')

"""#### WOE: `credit_pull_year`"""

dist('credit_pull_year')

df_fe['credit_pull_year_woe'] = pd.cut(df_fe['credit_pull_year'],10)
woe(df_fe,'credit_pull_year_woe')

"""##### Feature we will drop because have:
* Information value <0.02 (useless predictive)
* Information value > 0.5 (suspicious predictive)
* Feature that not make sense to bin
"""

drop_list = ['verification_status',
             'delinq_2yrs',
             'inq_last_6mths',
             'pub_rec',
             'out_prncp',
             'total_rec_int',
             'total_rec_late_fee',
             'recoveries',
             'collections_12_mths_ex_med',
             'acc_now_delinq',
             'tot_coll_amt',
             'tot_cur_bal',
             'total_rev_hi_lim']

print (f'Before feature engineering using WOE and IV ,we have {df_eda.shape[1]} column')

df_eda_fe = df_eda.copy()

df_eda_fe = df_eda_fe.drop(drop_list, axis=1)

print (f'After feature engineering using WOE and IV ,we have {df_eda_fe.shape[1]} column')

"""### Feature Encoding"""

df_encode = df_eda_fe.copy()

"""### Label Encoding"""

print(df_encode['grade'].unique())
print(df_encode['emp_length'].unique())
print(df_encode['home_ownership'].unique())
print(df_encode['initial_list_status'].unique())
print(df_encode['term'].unique())

df_encode['purpose'].value_counts()

df_encode['home_ownership'].value_counts()

"""`home_ownership` with values ANY and NONE will be combined with value OTHER because they have the same meaning

* Adjusting label `home_ownership`
* Label encoding `grade`
"""

# Replace label with same characteristic
target_dict = {'MORTGAGE':'MORTGAGE',
               'RENT':'RENT',
               'OWN':'OWN',
               'OTHER':'OTHER',
               'ANY':'OTHER',
               'NONE':'OTHER'}

df_encode["home_ownership"] = df_encode["home_ownership"].map(target_dict)

df_encode['term'] = np.where(df_encode['term']==36,0,1)

df_encode['initial_list_status'] = np.where(df_encode['initial_list_status']=='f',0,1)

"""### One Hot Encoding Categoric"""

for cat in [['home_ownership','purpose','emp_length','grade']]:
    onehots = pd.get_dummies(df_encode[cat], prefix = cat)

onehots.info()

"""### Numeric Feature Encode"""

num = df_encode.select_dtypes(include='number').columns

manual_bin = ['last_pymnt_amnt','revol_util','revol_bal','pymnt_time','term','loan_status','annual_inc']
auto_bin = num.drop(manual_bin)

feat_manual_bin = df_encode[manual_bin]
feat_auto_bin = df_encode[auto_bin]

# make a function
def make_bins(df, feature, cut):
    df[feature] = pd.cut(df[feature],cut)
    return df

# loan amnt
loan_amnt = make_bins(feat_auto_bin, 'loan_amnt',10)
loan_amnt_dum = pd.get_dummies(loan_amnt['loan_amnt'], prefix='loan_amnt')

# int_rate
int_rate = make_bins(feat_auto_bin, 'int_rate',10)
int_rate_dum = pd.get_dummies(int_rate['int_rate'], prefix='int_rate')

# dti
dti = make_bins(feat_auto_bin, 'dti', 10)
dti_dum = pd.get_dummies(dti['dti'], prefix='dti')

# open_acc
open_acc = make_bins(feat_auto_bin,'open_acc',10)
open_acc_dum = pd.get_dummies(open_acc['open_acc'], prefix='open_acc')

# total_acc
total_acc = make_bins(feat_auto_bin, 'total_acc', 7)
total_acc_dum = pd.get_dummies(total_acc['total_acc'], prefix='total_acc')

# total_pymnt
total_pymnt = make_bins(feat_auto_bin, 'total_pymnt', 10)
total_pymnt_dum = pd.get_dummies(total_pymnt['total_pymnt'], prefix='total_pymnt')

# credit_pull_yea
credit_pull_year = make_bins(feat_auto_bin,'credit_pull_year',10)
credit_pull_year_dum = pd.get_dummies(credit_pull_year['credit_pull_year'], prefix='credit_pull_year')

num_auto_bin = pd.concat([loan_amnt_dum,int_rate_dum,dti_dum,open_acc_dum,
                     total_acc_dum,total_pymnt_dum,credit_pull_year_dum],axis=1)

# revol_bal
feat_manual_bin['revol_bal_(0, 5000)'] = np.where((feat_manual_bin['revol_bal']>=0)&(feat_manual_bin['revol_bal']<=5000),1,0)
feat_manual_bin['revol_bal_(5000, 10000)'] =   np.where((feat_manual_bin['revol_bal']>5000)&(feat_manual_bin['revol_bal']<=10000),1,0)
feat_manual_bin['revol_bal_(10000, 15000)'] = np.where((feat_manual_bin['revol_bal']>10000)&(feat_manual_bin['revol_bal']<=15000),1,0)
feat_manual_bin['revol_bal_(> 15000)'] = np.where(feat_manual_bin['revol_bal']>15000,1,0)

# revol_util
feat_manual_bin['revol_util_(0, 20)'] = np.where((feat_manual_bin['revol_util']>=0)&(feat_manual_bin['revol_util']<=20),1,0)
feat_manual_bin['revol_util_(20, 40)'] = np.where((feat_manual_bin['revol_util']>20)&(feat_manual_bin['revol_util']<=40),1,0)
feat_manual_bin['revol_util_(40, 60)'] = np.where((feat_manual_bin['revol_util']>40)&(feat_manual_bin['revol_util']<=60),1,0)
feat_manual_bin['revol_util_(60, 80)'] = np.where((feat_manual_bin['revol_util']>60)&(feat_manual_bin['revol_util']<=80),1,0)
feat_manual_bin['revol_util_(> 80)'] = np.where(feat_manual_bin['revol_util']>80,1,0)

# last_pymnt_amnt
feat_manual_bin['last_pymnt_amnt_(0,500)'] = np.where((feat_manual_bin['last_pymnt_amnt']>=0)&(feat_manual_bin['last_pymnt_amnt']<=500),1,0)
feat_manual_bin['last_pymnt_amnt_(500, 1000)'] = np.where((feat_manual_bin['last_pymnt_amnt']>500)&(feat_manual_bin['last_pymnt_amnt']<=1000),1,0)
feat_manual_bin['last_pymnt_amnt_(1000, 1500)'] = np.where((feat_manual_bin['last_pymnt_amnt']>1000)&(feat_manual_bin['last_pymnt_amnt']<=1500),1,0)
feat_manual_bin['last_pymnt_amnt_(1500, 3500)'] = np.where((feat_manual_bin['last_pymnt_amnt']>1500)&(feat_manual_bin['last_pymnt_amnt']<=3500),1,0)
feat_manual_bin['last_pymnt_amnt_(> 3500)'] = np.where((feat_manual_bin['last_pymnt_amnt']>3500),1,0)

# pymnt_time
feat_manual_bin['pymnt_time_(0, 1)'] = np.where((feat_manual_bin['pymnt_time']>=0)&(feat_manual_bin['pymnt_time']<=1),1,0)
feat_manual_bin['pymnt_time_(1, 6)'] = np.where((feat_manual_bin['pymnt_time']>1)&(feat_manual_bin['pymnt_time']<=6),1,0)
feat_manual_bin['pymnt_time_(6, 12)'] = np.where((feat_manual_bin['pymnt_time']>6)&(feat_manual_bin['pymnt_time']<=12),1,0)
feat_manual_bin['pymnt_time_(> 12)'] = np.where((feat_manual_bin['pymnt_time']>12),1,0)

# annual_inc
feat_manual_bin['annual_inc_(low income)'] = np.where((feat_manual_bin['annual_inc']>=0)&(feat_manual_bin['annual_inc']<=50000),1,0)
feat_manual_bin['annual_inc_(mid income)'] = np.where((feat_manual_bin['annual_inc']>50000)&(feat_manual_bin['annual_inc']<=200000),1,0)
feat_manual_bin['annual_inc_(high income)'] = np.where((feat_manual_bin['annual_inc']>200000),1,0)

#Drop orginial Feature
feat_manual_bin = feat_manual_bin.drop(manual_bin, axis=1)

df_encoded = pd.concat([onehots,num_auto_bin,feat_manual_bin,df_encode['term'],df_encode['initial_list_status'],df_eda_fe['loan_status']],axis = 1)

df_encoded.head()

df_encoded.shape

"""## Modeling"""

df_model = df_encoded.copy()

"""## Library For Modeling"""

# Split Dataset
from sklearn.model_selection import train_test_split
# Balance data train using SMOTE
from imblearn.over_sampling import SMOTE
# logistic Regression
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,classification_report, roc_auc_score, roc_curve, auc, confusion_matrix
# Hyperparameter
from sklearn.model_selection import RandomizedSearchCV
#Confusion Matrix
def plot_confusion_matrix(cm,
                          target_names,
                          title='Confusion matrix',
                          cmap=None,
                          normalize=True):
    import matplotlib.pyplot as plt
    import numpy as np
    import itertools
    accuracy = np.trace(cm) / np.sum(cm).astype('float')
    misclass = 1 - accuracy
    if cmap is None:
        cmap = plt.get_cmap('Blues')
    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    if target_names is not None:
        tick_marks = np.arange(len(target_names))
        plt.xticks(tick_marks, target_names, rotation=45)
        plt.yticks(tick_marks, target_names)
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    thresh = cm.max() / 1.5 if normalize else cm.max() / 2
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        if normalize:
            plt.text(j, i, "{:0.4f}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")
        else:
            plt.text(j, i, "{:,}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))
    plt.show()

X = df_model.drop(['loan_status'], axis=1)
y = df_model['loan_status']

y.value_counts()

#Split Dataset 70% Train : 30% Test
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=24)
X_train.shape, X_test.shape, y_train.shape, y_test.shape

# Handle Imbalance Target Using SMOTE
sm = SMOTE(random_state=24)
sm.fit(X_train, y_train)
X_smote, y_smote = sm.fit_resample(X_train, y_train)
X_smote.shape, X_train.shape, y_smote.shape, y_train.shape

"""### Train Model

### Logistic Regression
"""

logreg = LogisticRegression(random_state = 24)
logreg.fit(X_smote, y_smote)

y_pred_proba_train = logreg.predict_proba(X_train)[:][:,1]
y_pred_proba_test = logreg.predict_proba(X_test)[:][:,1]

print('AUC Train Proba :', roc_auc_score(y_train, y_pred_proba_train))
print('AUC Test Proba :', roc_auc_score(y_test, y_pred_proba_test))

y_pred_class = []

for i in y_pred_proba_test:
    if i > 0.5:
        y_pred_class.append(1)
    else:
        y_pred_class.append(0)

print(classification_report(y_test, y_pred_class))

cm = confusion_matrix(y_test, y_pred_class)
target_names = ['Bad Loan','Good Loan']

plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=None,normalize=False)

"""#### Hyperparameter"""

param = {
    'penalty' : ['none', 'l2', 'l1', 'elasticnet'],
    'C' : [float(x) for x in np.linspace(start=0, stop=1, num=75)]
     }

logreg = LogisticRegression()

# search
logreg_clf = RandomizedSearchCV(logreg,
                            param,
                            scoring='roc_auc',
                            cv=5,
                            random_state = 24)

search_logreg = logreg_clf.fit(X_smote,y_smote)
# best hyperparameters
search_logreg.best_params_

"""## Retrain with hyperparameter tuning"""

best_params = search_logreg.best_params_
logreg_tuning = LogisticRegression(**best_params)
logreg_tuning.fit(X_smote,y_smote)
y_train_pred_proba = logreg_tuning.predict_proba(X_train)[:][:,1]
y_test_pred_lr_proba = logreg_tuning.predict_proba(X_test)[:][:,1]

print('AUC Train Proba :', roc_auc_score(y_train, y_train_pred_proba))
print('AUC Test Proba :', roc_auc_score(y_test, y_test_pred_lr_proba))

y_pred_class_2 = []

for i in y_test_pred_lr_proba:
    if i > 0.5:
        y_pred_class_2.append(1)
    else:
        y_pred_class_2.append(0)

print(classification_report(y_test, y_pred_class_2))

cm = confusion_matrix(y_test, y_pred_class_2)
target_names = ['Bad Loan','Good Loan']

plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=None,normalize=False)

"""## Show coeficient value each feature with statmodel logistic regression"""

import statsmodels.api as sm
X2 = sm.add_constant(X_smote)
est = sm.Logit(y_smote, X2)
est2 = est.fit(method='bfgs')
print(est2.summary())

# Converting statsmodels summary object to Pandas Dataframe,
df_importance = pd.read_html(est2.summary().tables[1].as_html(),header=0,index_col=0)[0]

# find odds_ratio
for i in df_importance['coef']:
    if i == 0 :
        df_importance['odds_ratio'] = 0
    else:
        df_importance['odds_ratio'] = np.exp(df_importance['coef'])

# show probability contribution
df_importance[df_importance['P>|z|'] <= 0.05].sort_values('odds_ratio',ascending = False)

"""## Show ROC Curve"""

fpr, tpr, tr = roc_curve(y_test, y_test_pred_lr_proba)
auc = roc_auc_score(y_test, y_test_pred_lr_proba)

plt.figure(figsize=(6,6))
plt.plot(fpr, tpr, label='AUC = %0.3f' %auc)
plt.plot(fpr, fpr, linestyle = '--', color='grey')
plt.xlabel('False Positive Rate', fontsize=10)
plt.ylabel('True Positive Rate', fontsize=10)
plt.title('ROC Curve', fontsize=15)
plt.legend()

"""## Show Kolmogrov-Smirnov"""

import scikitplot as skplt
y_pred_proba = logreg_tuning.predict_proba(X_test)

skplt.metrics.plot_ks_statistic(y_test, y_pred_proba, figsize=(7,5));

"""### Creating Score Card"""

# Set new index
df_importance = df_importance.reset_index()

# Rename columns
df_importance = df_importance.rename(columns = {'index' : 'feature'})

# Create new columns feature_name
df_importance['feature_name'] = df_importance['feature'].str.split('_').str[:-1]
df_importance['feature_name'] = df_importance['feature_name'].str.join('_')
df_importance.at[0,'feature_name']='intercept'
df_importance.at[125,'feature_name']='term'
df_importance.at[126,'feature_name']='initial_list_status'

df_importance

# copy dataset
df_scorecard = df_importance.copy()

# define max and min score
min_score = 300
max_score = 850

# aggregate min and sum
min_sum_coef = df_scorecard.groupby('feature_name')['coef'].min().sum()

# aggregate max and sum
max_sum_coef = df_scorecard.groupby('feature_name')['coef'].max().sum()

# define credit score
df_scorecard['Score_Calculation'] = df_scorecard['coef'] * (max_score - min_score) / (max_sum_coef - min_sum_coef)

# adjust intercept values
df_scorecard['Score_Calculation'][0] = ((df_scorecard['coef'][0] - min_sum_coef) / ((max_sum_coef - min_sum_coef)) * (max_score - min_score) + min_score)

# round credit score
df_scorecard['Score_Final'] = df_scorecard['Score_Calculation'].round()

# check min score (300) & max score (850)
min_sum_score_prel = df_scorecard.groupby('feature_name')['Score_Final'].min().sum()
max_sum_score_prel = df_scorecard.groupby('feature_name')['Score_Final'].max().sum()

print('min score', min_sum_score_prel)
print('max score', max_sum_score_prel)

df_scorecard.sort_values('Score_Final',ascending = False).head(10)

high_score= df_scorecard.sort_values('Score_Final',ascending = False).head(11)
fig, ax = plt.subplots(figsize=(15, 7))
sns.barplot(x='Score_Final', y='feature', data=high_score)
plt.bar_label(ax.containers[0], padding=-80)
plt.ylabel(' ')
plt.xlabel('Score')
plt.title('Top 10 High Score Features', fontsize=15, weight='extra bold')

"""* The features above make a high contribution to determining the increase in credit score and lead to be good loan"""

low_score= df_scorecard.sort_values('Score_Final',ascending = True).head(10)
fig, ax = plt.subplots(figsize=(15, 7))
sns.barplot(x='Score_Final', y='feature', data=low_score)
plt.bar_label(ax.containers[0], padding=-80)
plt.ylabel(' ')
plt.xlabel('Score')
plt.title('Top 10 Low Score Features', fontsize=15, weight='extra bold')

"""* While the above features can decrease high credit score because some of these features can lead to bad loans"""

# define data
data_fico = df_model[X_smote.columns]
df_score = data_fico.copy()
df_score.head()

df_score.insert(0, 'Intercept', 1)
scorecard_scores = df_scorecard['Score_Final']
# reshape
scorecard_scores = scorecard_scores.values.reshape(127, 1)
# Calculate Score with matrix multiplication
y_scores = df_score.dot(scorecard_scores)
# Concat
score_card_df = pd.concat([df_score, y_scores], axis=1)
# Rename
score_card_df.rename(columns={0:'Credit Score'}, inplace=True)

# show
score_card_df.head(5)

# Merge Score To Original Dataframe with id
df_id = df[['id','member_id']].copy()
credit_score_w_id = pd.merge(df_id, score_card_df, left_index = True, right_index = True)

credit_score_w_id

result_credit_score = credit_score_w_id[['id','member_id','Credit Score']]

result_credit_score.sample(10)

#result_credit_score.to_excel('Credit Score.xlsx')

# Merge Score To Raw Dataframe
credit_score_w_id_only = credit_score_w_id[['id','Credit Score']]
df_result = pd.merge(df,df_cleaned[['id','pymnt_time','credit_pull_year']],on = 'id')
df_result = pd.merge(df_result, credit_score_w_id_only,on='id')

df_result.sample(10)

"""## Visualization Based on feature importance model"""

df_vis = df_result.copy()

"""### Fico Score

![FICO_Credit_Score_Ranges.png](attachment:37edf5aa-c502-4963-b72c-689abf539502.png)

We will group credit score based on 'Forbes Advisor'.<br>
source = https://www.forbes.com/advisor/credit-score/what-is-a-fico-score/
"""

df_vis['score_group'] = np.where((df_vis['Credit Score']>=280)&(df_vis['Credit Score']<580),'Poor (300-579)',
                               np.where((df_vis['Credit Score']>= 580)&(df_vis['Credit Score']<670),'Fair (580-669)',
                               np.where((df_vis['Credit Score']>= 670)&(df_vis['Credit Score']<740),'Good (670-739)',
                               np.where((df_vis['Credit Score']>= 740)&(df_vis['Credit Score']<800),'Good (740-800)','Excellent (801-850)'))))

score_groupby = df_vis.groupby(['score_group']).agg(num_cust = ('id','count')).reset_index()
score_groupby['percentage'] = round((score_groupby ['num_cust']/466285)*100, 2)
score_groupby = score_groupby.sort_values('percentage',ascending = False)
score_groupby

fig, ax = plt.subplots(figsize=(12, 7))
sns.barplot(x = score_groupby['score_group'],y =score_groupby['percentage'],orient='v')
plt.bar_label(ax.containers[0], padding=5,fmt='%.2f%%')

"""* Feature `score_group` explain that majority of customer in this dataset was on credit score Poor group (300-579) by 99.20% while 0.80% on Fair (580-669).<br>
* No customer has credit score above 670 based on scoring using logistic regression. <br>
* Lenders must be careful in providing loans to customers and find solutions on how to take advantage of this situation.

### Adjusting Label to Visualization
"""

#Adjusting Label on loan_status
good_loan = ['Current','Fully Paid','In Grace Period']
df_vis['loan_status'] = np.where(df_vis['loan_status'].isin(good_loan),'Good Loan','Bad Loan')

#Adjusting Label on loan_amnt
df_vis['loan_amnt_fc'] = np.where((df_vis['loan_amnt']>=465.5)&(df_vis['loan_amnt']< 3950),'465-3950',
                            np.where((df_vis['loan_amnt']>= 3950)&(df_vis['loan_amnt']<7400),'3950-7400',
                            np.where((df_vis['loan_amnt']>= 7400.0)&(df_vis['loan_amnt']<10850.0),'7400-10850',
                            np.where((df_vis['loan_amnt']>= 10850.0)&(df_vis['loan_amnt']<14300.0),'10850-14300',
                            np.where((df_vis['loan_amnt']>= 14300.0)&(df_vis['loan_amnt']<17750.0),'14300-17750',
                            np.where((df_vis['loan_amnt']>= 17750.0)&(df_vis['loan_amnt']<21200.0),'17750-21200',
                            np.where((df_vis['loan_amnt']>= 21200.0)&(df_vis['loan_amnt']<24650.0),'21200-24650',
                            np.where((df_vis['loan_amnt']>= 24650.0)&(df_vis['loan_amnt']<28100.0),'24650-28100',
                            np.where((df_vis['loan_amnt']>= 28100.0)&(df_vis['loan_amnt']<31550.0),'28100-31550','31550-35000')))))))))

#Adjusting Label on last_pymnt_amnt
df_vis['last_pymnt_amnt_fc'] = np.where((df_vis['last_pymnt_amnt']>=0)&(df_vis['last_pymnt_amnt']< 500),'0-500',
                            np.where((df_vis['last_pymnt_amnt']>= 500)&(df_vis['last_pymnt_amnt']<1000),'500-1000',
                            np.where((df_vis['last_pymnt_amnt']>= 1000)&(df_vis['last_pymnt_amnt']<1500),'1000-1500',
                            np.where((df_vis['last_pymnt_amnt']>= 1500)&(df_vis['last_pymnt_amnt']<3500),'1500-3500','> 3500'))))

#Adjusting Label on pymnt_time
df_vis['pymnt_time_fc'] = np.where((df_vis['pymnt_time']>=0)&(df_vis['pymnt_time']<=1),'1 Month',
                            np.where((df_vis['pymnt_time']> 1)&(df_vis['pymnt_time']<=6),'2-6 Month',
                            np.where((df_vis['pymnt_time']> 6)&(df_vis['pymnt_time']<=12),'7-12 Month','Over 1 Year')))

#Adjusting Label on int_rate
df_vis['int_rate_fc'] = np.where((df_vis['int_rate']>=5.399)&(df_vis['int_rate']< 7.484),'5.3-7.4',
                            np.where((df_vis['int_rate']>= 7.484)&(df_vis['int_rate']<9.548),'7.5-9.5',
                            np.where((df_vis['int_rate']>= 9.548)&(df_vis['int_rate']<11.612),'9.6-11.6',
                            np.where((df_vis['int_rate']>= 11.612)&(df_vis['int_rate']<13.676),'11.7-13.6',
                            np.where((df_vis['int_rate']>= 13.676)&(df_vis['int_rate']<15.74),'13.7-15.7',
                            np.where((df_vis['int_rate']>= 15.74)&(df_vis['int_rate']<17.804),'15.8-17.8',
                            np.where((df_vis['int_rate']>= 17.804)&(df_vis['int_rate']<19.868),'17.9-19.8',
                            np.where((df_vis['int_rate']>= 19.868)&(df_vis['int_rate']<21.932),'19.9-21.9',
                            np.where((df_vis['int_rate']>= 21.932)&(df_vis['int_rate']<23.996),'22-23.9','24-26')))))))))

"""#### Create Orderlist"""

order_list_loan = ['465-3950','3950-7400','7400-10850','10850-14300','14300-17750',
                  '17750-21200','21200-24650','24650-28100','28100-31550','31550-35000']
order_list_last_pymnt = ['0-500','500-1000','1000-1500','1500-3500','> 3500']
order_list_pymnt_time = ['1 Month','2-6 Month','7-12 Month','Over 1 Year']
order_list_int = ['5.3-7.4','7.5-9.5','9.6-11.6','11.7-13.6','13.7-15.7',
                  '15.8-17.8','17.9-19.8','19.9-21.9','22-23.9','24-26']
order_list_emp = ['< 1 year','1 year','2 years','3 years','4 years','5 years','6 years','7 years','8 years','9 years','10+ years']

"""### Create Visual

### Loan Status
"""

loan_status_groupby = df_vis.groupby(['loan_status']).agg(num_cust = ('id','count')).reset_index()
loan_status_groupby['percentage'] = round((loan_status_groupby ['num_cust']/466285)*100, 2)
loan_status_groupby = loan_status_groupby.sort_values('percentage',ascending = False)
loan_status_groupby

fig, ax = plt.subplots(figsize=(12, 7))
sns.barplot(x = loan_status_groupby['loan_status'],y =loan_status_groupby['percentage'],orient='v')
plt.bar_label(ax.containers[0], padding=5,fmt='%.2f%%')
plt.title('Percentage Each Loan Status',
          fontsize=25, weight='extra bold', pad=30)
plt.ylim(0,100)
plt.ylabel('Percentage (%)', fontsize=14)
plt.xlabel('Score Group', fontsize=14)

"""#### Bad Loan Rate Based On Loan Amount"""

df_vis_loan = df_vis.groupby(['score_group','loan_status','loan_amnt_fc']).agg(num_cust=('id','count')).reset_index()
total_cust_loan= df_vis_loan.groupby(['loan_amnt_fc']).agg(total_cust=('num_cust','sum')).reset_index()
df_vis_loan_group = df_vis_loan.merge(total_cust_loan,on = ['loan_amnt_fc'])
bad_loan_rate = df_vis_loan_group[df_vis_loan_group['loan_status']=='Bad Loan']
bad_loan_rate['bad_loan_rate'] = round((bad_loan_rate['num_cust']/bad_loan_rate['total_cust'])*100, 2)
bad_loan_rate

plt.style.use('fivethirtyeight')
fig, ax = plt.subplots(figsize=(20, 10))

sns.barplot(x='loan_amnt_fc',y='bad_loan_rate',data=bad_loan_rate,palette='rocket',
              hue='score_group',ci=None, order=order_list_loan)

plt.axvline(7.5, ls='--', color='indianred')
plt.axvline(9.5, ls='--', color='indianred')
plt.stackplot(np.arange(7.5,10), [[25000]], color='indianred', alpha=0.3)
plt.text(x=7.6, y=18, s='This Loan Is Too Risky', fontsize=15,
         color='#ff0f0f', va='center', weight='extra bold')
#plt.text(x=8, y=17, s='', fontsize=9,
         #color='#ff0f0f', va='center', weight='extra bold')


plt.ylim(0,20)

plt.title('Bad Loan Rate On Loan Amount \nBased On Borrowers Score Status',
          fontsize=18, weight='extra bold', pad=70)
plt.text(x=-0.9, y=21, s='Customer who have credit score on Poor (300-579) with borrow loan amount 28100-35000 have a high risk of becoming a bad loan \nWhile good thing that customer who have credit score Fair (580-669) not have bad loan rate more than 1% ',
         fontstyle='italic', fontsize=14)
plt.bar_label(ax.containers[0], padding=5,fmt='%.2f%%')
plt.bar_label(ax.containers[1], padding=5,fmt='%.2f%%')

plt.legend(title='Status', loc='upper left')
plt.ylabel('Bad Loan Rate (%)', fontsize=14)
plt.xlabel('Loan Amount', fontsize=14)

"""#### Bad Loan Rate  Last Payment Amount"""

df_vis_payment = df_vis.groupby(['score_group','loan_status','last_pymnt_amnt_fc']).agg(num_cust=('id','count')).reset_index()
total_cust_payment= df_vis_payment.groupby(['last_pymnt_amnt_fc']).agg(total_cust=('num_cust','sum')).reset_index()
df_vis_payment_group = df_vis_payment.merge(total_cust_payment,on = ['last_pymnt_amnt_fc'])
bad_payment_rate = df_vis_payment_group[df_vis_payment_group['loan_status']=='Bad Loan']
bad_payment_rate['bad_loan_rate'] = round((bad_payment_rate['num_cust']/bad_payment_rate['total_cust'])*100, 2)
bad_payment_rate

plt.style.use('fivethirtyeight')
fig, ax = plt.subplots(figsize=(15, 7))

sns.barplot(x='last_pymnt_amnt_fc',y='bad_loan_rate',data=bad_payment_rate,palette='rocket',
              hue='score_group',ci=None, order=order_list_last_pymnt)

plt.bar_label(ax.containers[0], padding=5, fmt='%.2f%%')
plt.bar_label(ax.containers[1], padding=5, fmt='%.2f%%')

plt.ylim(0,25)
plt.title('Bad Loan Rate On Last Payment Amount \nBased On Borrowers Score Status',
          fontsize=18, weight='extra bold', pad=100)
plt.text(x=-0.4, y=27.5, s='More payment amount that customer pay,more low risk to be bad loan\nCustomer who have credit score poor group (300-579) with payment amount more than 3500 less likely to be a bad loan\nIdeally,lending company can set minimum payment amount for loan start from 1500 to decrease bad loan rate',
         fontstyle='italic', fontsize=14)
plt.legend(title='Score Group', loc='upper right')
plt.ylabel('Bad Loan Rate (%)', fontsize=14)
plt.xlabel('Last Payment Amount', fontsize=14)

"""#### Bad Loan Rate  Based On Interest Rate"""

df_vis_int = df_vis.groupby(['score_group','loan_status','int_rate_fc']).agg(num_cust=('id','count')).reset_index()
total_cust_int = df_vis_int.groupby(['int_rate_fc']).agg(total_cust=('num_cust','sum')).reset_index()
df_vis_int_group = df_vis_int.merge(total_cust_int,on = ['int_rate_fc'])
bad_int_rate = df_vis_int_group[df_vis_int_group['loan_status']=='Bad Loan']
bad_int_rate['bad_loan_rate'] = round((bad_int_rate['num_cust']/bad_int_rate['total_cust'])*100, 2)
bad_int_rate

plt.style.use('fivethirtyeight')
fig, ax = plt.subplots(figsize=(15, 7))

sns.barplot(x='int_rate_fc',y='bad_loan_rate',data=bad_int_rate,palette='rocket',
              hue='score_group',ci=None, order=order_list_int)


sns.regplot(x=np.arange(0, len(bad_int_rate[bad_int_rate['score_group'] == 'Fair (580-669)'])), y='bad_loan_rate',
            data=bad_int_rate[bad_int_rate['score_group'] == 'Fair (580-669)'], scatter=False, label = 'Trend Fair', truncate=False)
sns.regplot(x=np.arange(0, len(bad_int_rate[bad_int_rate['score_group'] == 'Poor (300-579)'])), y='bad_loan_rate',
            data=bad_int_rate[bad_int_rate['score_group'] == 'Poor (300-579)'], scatter=False, label = 'Trend Poor', truncate=False)

plt.bar_label(ax.containers[0], padding=5, fmt='%.2f%%')
plt.bar_label(ax.containers[1], padding=5, fmt='%.2f%%')

#plt.ylim(0,100)
plt.title('Positive Trend on Bad Loan Rate \nBased On Interest Rate',
          fontsize=18, weight='extra bold', pad=80)
plt.text(x=-0.85, y=28.5, s='More Interest rate that customer take,more risk to be bad loan\nCustomer who have credit score poor group (300-579) isnt good to take interest rate more than 20%\nIdeally interest rate below 14% is maksimum that lender can offer to customer because bad loan rate still under 10%',
         fontstyle='italic', fontsize=14)
#plt.legend(title='Status', loc='upper left')
plt.ylabel('Bad Loan Rate (%)', fontsize=14)
plt.xlabel('Interest Rate (%)', fontsize=14)

"""#### Bad Loan Rate  Based On Payment Time"""

df_vis_pymnt_time = df_vis.groupby(['pymnt_time_fc','score_group','loan_status']).agg(num_cust=('id','count')).reset_index()
total_cust_pymnt= df_vis_pymnt_time.groupby(['pymnt_time_fc']).agg(total_cust=('num_cust','sum')).reset_index()
df_vis_pymnt_group = df_vis_pymnt_time.merge(total_cust_pymnt,on = ['pymnt_time_fc'])
bad_pymnt_rate = df_vis_pymnt_group[df_vis_pymnt_group['loan_status']=='Bad Loan']
bad_pymnt_rate['bad_loan_rate'] = round((bad_pymnt_rate['num_cust']/bad_pymnt_rate['total_cust'])*100, 2)

plt.style.use('fivethirtyeight')
fig, ax = plt.subplots(figsize=(15, 7))

sns.barplot(x='pymnt_time_fc',y='bad_loan_rate',data=bad_pymnt_rate,palette='rocket',
              hue='score_group',ci=None, order=order_list_pymnt_time)


sns.regplot(x=np.arange(0, len(bad_pymnt_rate[bad_pymnt_rate['score_group'] == 'Fair (580-669)'])), y='bad_loan_rate',
            data=bad_pymnt_rate[bad_pymnt_rate['score_group'] == 'Fair (580-669)'], scatter=False, label = 'Trend Fair', truncate=False)
sns.regplot(x=np.arange(0, len(bad_pymnt_rate[bad_pymnt_rate['score_group'] == 'Poor (300-579)'])), y='bad_loan_rate',
            data=bad_pymnt_rate[bad_pymnt_rate['score_group'] == 'Poor (300-579)'], scatter=False, label = 'Trend Poor', truncate=False)

plt.bar_label(ax.containers[0], padding=5, fmt='%.2f%%')
plt.bar_label(ax.containers[1], padding=5, fmt='%.2f%%')

#plt.ylim(0,100)
plt.title('Trend on Bad Loan Rate \nBased On Payment Time',
          fontsize=18, weight='extra bold', pad=70)
plt.text(x=-0.55, y=40, s='The longer the payment due by the borrower, the higher the risk of the borrower becoming a bad loan\nPayment time more than 6 Month increase the risk of bad loans by up to 20% for customer who have score credit on Poor (300-579)',
         fontstyle='italic', fontsize=14)
plt.legend(title='Status', loc='upper left')
plt.ylabel('Bad Loan Rate (%)', fontsize=14)
plt.xlabel('Payment Time', fontsize=14)

"""#### Good Loan Rate Based On Employment Length"""

df_vis_emp = df_vis.groupby(['emp_length','score_group','loan_status']).agg(num_cust=('id','count')).reset_index()
total_cust_emp = df_vis_emp.groupby(['emp_length']).agg(total_cust=('num_cust','sum')).reset_index()
df_vis_emp_group = df_vis_emp.merge(total_cust_emp,on = ['emp_length'])
bad_emp_rate = df_vis_emp_group[df_vis_emp_group['loan_status']=='Bad Loan']
bad_emp_rate['bad_loan_rate'] = round((bad_emp_rate['num_cust']/bad_emp_rate['total_cust'])*100, 2)

plt.style.use('fivethirtyeight')
fig, ax = plt.subplots(figsize=(15, 7))

sns.barplot(x='emp_length',y='bad_loan_rate',data=bad_emp_rate,palette='rocket',
              hue='score_group',ci=None, order=order_list_emp)


sns.regplot(x=np.arange(0, len(bad_emp_rate[bad_emp_rate['score_group'] == 'Fair (580-669)'])), y='bad_loan_rate',
            data=bad_emp_rate[bad_emp_rate['score_group'] == 'Fair (580-669)'], scatter=False, label = 'Trend Fair', truncate=False)
sns.regplot(x=np.arange(0, len(bad_emp_rate[bad_emp_rate['score_group'] == 'Poor (300-579)'])), y='bad_loan_rate',
            data=bad_emp_rate[bad_emp_rate['score_group'] == 'Poor (300-579)'], scatter=False, label = 'Trend Poor', truncate=False)

plt.bar_label(ax.containers[0], padding=5, fmt='%.2f%%')
plt.bar_label(ax.containers[1], padding=5, fmt='%.2f%%')

plt.ylim(0,22)
plt.title('Trend on Bad Loan Rate \nBased On Employment Length',
          fontsize=22, weight='extra bold', pad=70)
plt.text(x=-0.85, y=23, s='The longer employment length that customer have, the lower the risk of the borrower becoming a bad loan\nEmployment length more than 10 Year decrease the risk of bad loans by 10.39% for customer who have score credit on Poor (300-579)',
         fontstyle='italic', fontsize=14)
plt.legend(title='Status', loc='upper left')
plt.ylabel('Bad Loan Rate (%)', fontsize=14)
plt.xlabel('Payment Time', fontsize=14)

"""## Summary

Lending Companies (LC) can pay attention to customer profile that is important enough to be considered for a loan, feature like:
1. Loan Amount; The amount of the loan given is related to the interest rate that must be paid. The larger the loan amount, the higher the interest rate that must be paid. The company can recommend a loan amount that is not too risky to become a bad loan.Loan amount more than 28100 not recomend to offer for customer.
2. Last Payment Amount; More payment amount that customer take, lower the risk of the customer becoming a bad loan.Lending Companies can set limit minimum amount from 1500 for payment amount that must customer pay every due
3. Payment Time; The longer time that must be paid by the customer, the higher the risk of the customer becoming a bad loan.Limiting the payment time max 6 years can reduce the risk of bad loans
4. Interest Rate; More interest rate that customer take,increasing more bad loan rate. Ideally if lending companies want to keep bad loan low, they can offer interest rate below 14%.Lending companies mus avoid to offer loan with interest rate more than 20%.
5. Employment Length; In the visualization, it has been proven that the longer the customer's work experience, the more capable the customer is to repay the loan, thereby increasing the good loan.
"""